{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a83392",
   "metadata": {},
   "source": [
    "# Projet IF29 P22\n",
    "### Détection des influenceurs sur Twitter en appliquant les méthodes non supervisée et supervisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87989867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la BD en utilisant le fichier 'Worldcup 200Tweets' sur moodle\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['______'] # nom de BD\n",
    "collection = db['______'] #nom de collection\n",
    "parent_dir = (\"_____________________________________\") #adresse de fichier\n",
    "list_filename = os.listdir(parent_dir)\n",
    "\n",
    "for filename in list_filename:\n",
    "    filepath = parent_dir + filename\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    for line in lines:\n",
    "        document = json.loads(line[:-1])\n",
    "        collection.insert_one(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87508bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecter à la BD\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['if29_projet']\n",
    "collection = db['if29_projet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b1d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dataframe en utilisant les attributs qui nous intéressent concernant les utilisateurs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cursor = collection.aggregate(\n",
    "    [\n",
    "        {\"$group\" : {\n",
    "            \"_id\" : \"$user.id\", \n",
    "            \"friends_count\" : {\"$last\" : \"$user.friends_count\"},\n",
    "            \"favourites_count\" : {\"$last\" : \"$user.favourites_count\"},\n",
    "            \"statuses_count\" : {\"$last\" : \"$user.statuses_count\"},\n",
    "            \"followers_count\" : {\"$last\" : \"$user.followers_count\"},\n",
    "             \"created_time\" : {\"$last\" : \"$user.created_at\"}\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df = pd.json_normalize(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b9ca2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>followers_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>267468</td>\n",
       "      <td>267468</td>\n",
       "      <td>267468</td>\n",
       "      <td>267468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1101</td>\n",
       "      <td>9108</td>\n",
       "      <td>22204</td>\n",
       "      <td>7829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5539</td>\n",
       "      <td>25271</td>\n",
       "      <td>54745</td>\n",
       "      <td>181738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>186</td>\n",
       "      <td>380</td>\n",
       "      <td>1329</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>408</td>\n",
       "      <td>1946</td>\n",
       "      <td>6275</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>906</td>\n",
       "      <td>7547</td>\n",
       "      <td>21751</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>544830</td>\n",
       "      <td>1718515</td>\n",
       "      <td>8898917</td>\n",
       "      <td>30755372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       friends_count  favourites_count  statuses_count  followers_count\n",
       "count         267468            267468          267468           267468\n",
       "mean            1101              9108           22204             7829\n",
       "std             5539             25271           54745           181738\n",
       "min                0                 0               1                0\n",
       "25%              186               380            1329              124\n",
       "50%              408              1946            6275              363\n",
       "75%              906              7547           21751             1019\n",
       "max           544830           1718515         8898917         30755372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Première analyse sur le résumé des données quantitatives\n",
    "df.iloc[:,[1,2,3,4]].describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3e4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les indicateurs (attributs dérivés) ci-dessous destinés à la description des 'influenceurs'\n",
    "# 1) Visibilité: Vis\n",
    "# 2) Nombre moyen de 'aimer' par tweet: avg_fav \n",
    "# 3) Ratio entre nombre de 'abonné' et nombre de 'abonnement': r_fri_follow\n",
    "# 4) la fréquence de publication de tweets depuis la création de compte jusqu'à t0 = 01/01/2019: frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2655a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Visibilité: Vis\n",
    "\n",
    "# Regrouper les textes de tweet par chaque utilisateur\n",
    "cursor1 = collection.aggregate(\n",
    "    [        \n",
    "         {\"$group\" : {\n",
    "            \"_id\" : \"$user.id\", \n",
    "            \n",
    "            \"tweets\" : {\"$push\": \"$text\"}\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df1 = pd.json_normalize(cursor1)\n",
    "\n",
    "def calcul_visibility(tweets):\n",
    "    s = 0\n",
    "    for tweet in tweets:\n",
    "        s += tweet.count(\"@\")*11.4  # le coût moyen pour @ est 11,4\n",
    "        s += tweet.count(\"#\")*11.6  # le coût moyen pour # est 11,6 cf.fichier 'SPOT'\n",
    "    return s/(140*len(tweets))\n",
    "\n",
    "visibilities = []\n",
    "for texts in df1.iloc[:,1]:\n",
    "    visibilities.append(calcul_visibility(texts)) # mettre les résultats dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5d5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Nombre moyen de 'aimer' par tweet: avg_fav\n",
    "avg_fav = df.favourites_count/df.statuses_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab2df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Ratio entre nombre de 'abonné' et nombre de 'abonnement': r_fri_follow\n",
    "r_fri_follow = df.followers_count/df.friends_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1f2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) la fréquence de publication de tweets depuis la création de compte jusqu'à t0 = 01/01/2019: frequency\n",
    "projection = { \n",
    "    'user.statuses_count': 1,\n",
    "    'user.created_at': 1,\n",
    "    '_id': 0}\n",
    "cursor3 = collection.find({},projection)\n",
    "\n",
    "import time\n",
    "def ratio(n_tweets, date):\n",
    "    time_array = time.strptime(date, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    time_stamp = time.mktime(time_array)\n",
    "    time_stamp_0 = time.mktime(time.strptime(\"Mon Jan 1 00:00:00 2019\", '%a %b %d %H:%M:%S %Y'))\n",
    "    return n_tweets/(time_stamp_0 - time_stamp)\n",
    "\n",
    "frequency = []\n",
    "for index, row in df.iterrows():\n",
    "    n_tweet = row[\"statuses_count\"]\n",
    "    created_date = row[\"created_time\"]\n",
    "    frequency.append(ratio(n_tweet, created_date)*100) # on multiplie une constante = 100 pour amplifier les résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer un dataframe en regroupant les quatres indicateurs calculés\n",
    "features = pd.DataFrame()\n",
    "features[\"user.id\"] = df.iloc[:,[0]]\n",
    "features[\"vis\"] = visibilities\n",
    "features[\"r_fri_follow\"] = r_fri_follow\n",
    "features[\"avg_fav\"] = avg_fav\n",
    "features[\"frequency\"] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc96eeee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# On supprime toutes les lignes qui contiennent 'Nan' ou 'inf'\n",
    "features.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On normalise les données pour effectuer les classifications\n",
    "from sklearn import preprocessing\n",
    "\n",
    "features_normalized = preprocessing.normalize(features.iloc[:,[1,2,3,4]])\n",
    "\n",
    "# Vérification\n",
    "print(\"La moyenne : \",np.mean(features_normalized,axis=0))\n",
    "print(\"L'écart type : \",np.std(features_normalized,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f21f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On effectue l'ACP pour réduire les 4 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "acp = PCA(svd_solver='auto')\n",
    "coord = acp.fit_transform(features_normalized) \n",
    "acp.explained_variance_ratio_ # les pourcentage de variance projectée sur chaque axe (composante principale)\n",
    "coord # les coordonées des individus dans les nouveaux repères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basé sur le résultat d'ACP, on effectue le K-means dans le plan (1,2) qui se compose de deux composantes dont inerties les plus élevées \n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0) # Selon le sujet, ici on ne choisit que de classer en 2 clusters.\n",
    "labels = kmeans.fit_predict(coord[:,[0,1]]) # on ne prend que les coordonées de l'axe 1 et 2\n",
    "\n",
    "np.unique(labels, return_counts=True) # afficher la condition de séparation\n",
    "\n",
    "labels0 = coord[labels == 0]\n",
    "labels1 = coord[labels == 1]\n",
    "plt.scatter(np.random.choice(labels0[:,0],100), np.random.choice(labels0[:,1],100), c='red') # Pour voir plus clairement, on prend aléatoirement 100 points pour tracer le graphique\n",
    "plt.scatter(np.random.choice(labels1[:,0],100), np.random.choice(labels1[:,1],100), c='blue') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de passer à l'approche supervisée, il faut d'abord labeliser les données\n",
    "# Dans cette étape on va utiliser un échantillon de taille 50000 pour entraîner le modèle\n",
    "# Comme on a 4 indicateurs, on décide de les combiner en utilisant la méthode d'entropie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82985f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode d'entropie\n",
    "\n",
    "data = features.iloc[:,[1,2,3,4]]\n",
    "indicateur = data.columns.tolist()   ## nombre des indicateurs\n",
    "profil = data.index.tolist()    ## nombre des profils\n",
    "value = data.values\n",
    "\n",
    "# définir la fonction de normalisation, on rajoute une constante 0,01\n",
    "def std_data(value,flag):\n",
    "    for i in range(len(indicateur)):\n",
    "        if flag[i]=='+':\n",
    "            value[:,i]=(value[:,i]-np.min(value[:,i],axis=0))/(np.max(value[:,i],axis=0)-np.min(value[:,i],axis=0))+0.01\n",
    "        elif flag[i]=='-':\n",
    "            value[:,i]=(np.max(value[:,i],axis=0)-value[:,i])/(np.max(value[:,i],axis=0)-np.min(value[:,i],axis=0))+0.01\n",
    "    return value\n",
    "\n",
    "# définir la fonction d'entropie et calculer les poids et les entropies\n",
    "def cal_weight(indicateur,profil,value):\n",
    "    p= np.array([[0.0 for i in range(len(indicateur))] for i in range(len(profil))])                    \n",
    "    for i in range(len(indicateur)):\n",
    "        p[:,i]=value[:,i]/np.sum(value[:,i],axis=0)\n",
    "        \n",
    "    e=-1/np.log(len(profil))*sum(p*np.log(p))      #calcul d'entropie\n",
    "    g=1-e     \n",
    "    w=g/sum(g)     #calcul des poids\n",
    "    return w\n",
    "\n",
    "# normalisation des données\n",
    "flag=[\"+\",\"+\",\"+\",\"+\"]  ## '+' ou '-' désigne le sens positif ou négatif des indicateurs\n",
    "std_value=std_data(value,flag)\n",
    "std_value.round(3)\n",
    "\n",
    "# Résultat\n",
    "w=cal_weight(indicateur,profil,std_value)\n",
    "w=pd.DataFrame(w,index=data.columns,columns=['Poids'])\n",
    "print(\"#######Poids:#######\")\n",
    "print(w)\n",
    "score=np.dot(std_value,w).round(2)\n",
    "score=pd.DataFrame(score,index=data.index,columns=['Notes']).sort_values(by =['Notes'],ascending = False)\n",
    "\n",
    "# la note de chaque utilisateur représente la correspondance avec les profils qu'on va détecter\n",
    "# Si une note plus élevée, le profil correspondant est plus probablement censé un influnceur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c878cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A l'aide de résultat ci-dessus, on peut labeliser les profils "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
