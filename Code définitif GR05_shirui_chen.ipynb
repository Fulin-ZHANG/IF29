{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a83392",
   "metadata": {},
   "source": [
    "# Projet IF29 P22\n",
    "### Détection des influenceurs sur Twitter en appliquant les méthodes non supervisée et supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295507f",
   "metadata": {},
   "source": [
    "## Inserer les donnees dans MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785102f",
   "metadata": {},
   "source": [
    "### Créer la BD en utilisant le fichier 'Worldcup 200Tweets' sur moodle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16011e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c20bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_TIME_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87989867",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_TIME_RUN:\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['if29_projet'] # nom de BD\n",
    "    collection = db['if29_projet'] #nom de collection\n",
    "    parent_dir = (\"raw/\") #adresse de fichier\n",
    "    list_filename = os.listdir(parent_dir)\n",
    "\n",
    "    for filename in list_filename:\n",
    "        filepath = parent_dir + filename\n",
    "        with open(filepath, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "        for line in lines:\n",
    "            document = json.loads(line[:-1])\n",
    "            collection.insert_one(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af232eff",
   "metadata": {},
   "source": [
    "## Connecter à la BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87508bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_TIME_RUN:\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['if29_projet']\n",
    "    collection = db['if29_projet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872a2ba",
   "metadata": {},
   "source": [
    "## Créer le dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454334a",
   "metadata": {},
   "source": [
    "### Créer le dataframe en utilisant les attributs qui nous intéressent concernant les utilisateurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b1d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_TIME_RUN:\n",
    "    cursor = collection.aggregate(\n",
    "        [\n",
    "            {\"$group\" : {\n",
    "                \"_id\" : \"$user.id\", \n",
    "                \"friends_count\" : {\"$last\" : \"$user.friends_count\"},\n",
    "                \"favourites_count\" : {\"$last\" : \"$user.favourites_count\"},\n",
    "                \"statuses_count\" : {\"$last\" : \"$user.statuses_count\"},\n",
    "                \"followers_count\" : {\"$last\" : \"$user.followers_count\"},\n",
    "                 \"created_time\" : {\"$last\" : \"$user.created_at\"}\n",
    "            }\n",
    "            }\n",
    "        ],\n",
    "        allowDiskUse=True,\n",
    "    )\n",
    "    df = pd.json_normalize(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449bd1c",
   "metadata": {},
   "source": [
    "### If we run the program for the first time, we pickle notre pandas dataframe, comme ca on n'a pas besoin de connecter au MongoDB a chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4acaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_TIME_RUN:\n",
    "    df.to_pickle('twitter.pkl', protocol=4)\n",
    "else:\n",
    "    df = pd.read_pickle(\"twitter.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298acd9",
   "metadata": {},
   "source": [
    "### Première analyse sur le résumé des données quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b9ca2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>followers_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843438</td>\n",
       "      <td>1843438</td>\n",
       "      <td>1843438</td>\n",
       "      <td>1843438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>870</td>\n",
       "      <td>8753</td>\n",
       "      <td>17675</td>\n",
       "      <td>3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4889</td>\n",
       "      <td>22033</td>\n",
       "      <td>40482</td>\n",
       "      <td>118540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>168</td>\n",
       "      <td>443</td>\n",
       "      <td>1104</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>357</td>\n",
       "      <td>2164</td>\n",
       "      <td>5204</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>758</td>\n",
       "      <td>7922</td>\n",
       "      <td>17885</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1569366</td>\n",
       "      <td>1718515</td>\n",
       "      <td>8909333</td>\n",
       "      <td>42198786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       friends_count  favourites_count  statuses_count  followers_count\n",
       "count        1843438           1843438         1843438          1843438\n",
       "mean             870              8753           17675             3601\n",
       "std             4889             22033           40482           118540\n",
       "min                0                 0               1                0\n",
       "25%              168               443            1104              103\n",
       "50%              357              2164            5204              298\n",
       "75%              758              7922           17885              753\n",
       "max          1569366           1718515         8909333         42198786"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,[1,2,3,4]].describe().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f1e40",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526353f7",
   "metadata": {},
   "source": [
    "### Calculer les indicateurs (attributs dérivés) ci-dessous destinés à la description des 'influenceurs'\n",
    "- 1) Visibilité: Vis\n",
    "- 2) Nombre moyen de 'aimer' par tweet: avg_fav \n",
    "- 3) Ratio entre nombre de 'abonné' et nombre de 'abonnement': r_fri_follow\n",
    "- 4) la fréquence de publication de tweets depuis la création de compte jusqu'à t0 = 01/01/2019: frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9247b62",
   "metadata": {},
   "source": [
    "### 1) Visibilité: Vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58553888",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_TIME_RUN:\n",
    "    # Regrouper les textes de tweet par chaque utilisateur\n",
    "    cursor1 = collection.aggregate(\n",
    "        [        \n",
    "             {\"$group\" : {\n",
    "                \"_id\" : \"$user.id\", \n",
    "\n",
    "                \"tweets\" : {\"$push\": \"$text\"}\n",
    "            }\n",
    "            }\n",
    "        ],\n",
    "        allowDiskUse=True\n",
    "    )\n",
    "    df1 = pd.json_normalize(cursor1)\n",
    "    df1.to_pickle('df1.pkl', protocol=4)\n",
    "else:\n",
    "    df1 = pd.read_pickle('df1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2655a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_visibility(row):\n",
    "    s = 0\n",
    "    for tweet in row[\"tweets\"]:\n",
    "        s += tweet.count(\"@\")*11.4  # le coût moyen pour @ est 11,4\n",
    "        s += tweet.count(\"#\")*11.6  # le coût moyen pour # est 11,6 cf.fichier 'SPOT'\n",
    "    return s/(140*len(row[\"tweets\"]))\n",
    "\n",
    "visibilities = df1.apply(calcul_visibility,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b843522",
   "metadata": {},
   "source": [
    "### 2) Nombre moyen de 'aimer' par tweet: avg_fav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5d5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fav = df.favourites_count/df.statuses_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34258d1",
   "metadata": {},
   "source": [
    "### 3) Ratio entre nombre de 'abonné' et nombre de 'abonnement': r_fri_follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab2df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_fri_follow = df.followers_count/df.friends_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b56811",
   "metadata": {},
   "source": [
    "### 4) la fréquence de publication de tweets depuis la création de compte jusqu'à t0 = 01/01/2019: frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fba8c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "mktime argument out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-47238f689ab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"statuses_count\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_stamp_0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime_stamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mfrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\programs\\python370\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7551\u001b[0m         )\n\u001b[1;32m-> 7552\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python370\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python370\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python370\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-47238f689ab3>\u001b[0m in \u001b[0;36mratio\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtime_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"created_time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%a %b %d %H:%M:%S +0000 %Y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtime_stamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmktime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtime_stamp_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmktime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mon Jan 1 00:00:00 2019\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%a %b %d %H:%M:%S %Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# on multiplie une constante = 100 pour amplifier les résultats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: mktime argument out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def ratio(row):\n",
    "    time_array = time.strptime(row[\"created_time\"], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    time_stamp = time.mktime(time_array)\n",
    "    time_stamp_0 = time.mktime(time.strptime(\"Mon Jan 1 00:00:00 2019\", '%a %b %d %H:%M:%S %Y'))\n",
    "    # on multiplie une constante = 100 pour amplifier les résultats\n",
    "    return row[\"statuses_count\"]*100/(time_stamp_0 - time_stamp) \n",
    "\n",
    "frequency = df.apply(ratio, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83126c32",
   "metadata": {},
   "source": [
    "### Générer un dataframe en regroupant les quatres indicateurs calculés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features[\"user_id\"] = df.iloc[:,0]\n",
    "features[\"vis\"] = visibilities\n",
    "features[\"r_fri_follow\"] = r_fri_follow\n",
    "features[\"avg_fav\"] = avg_fav\n",
    "features[\"frequency\"] = frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176c858",
   "metadata": {},
   "source": [
    "### On supprime toutes les lignes qui contiennent 'Nan' ou 'inf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96eeee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True\n",
    "features.dropna(inplace=True)\n",
    "features = features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d68ef",
   "metadata": {},
   "source": [
    "### On normalise les données pour effectuer les classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f731b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "features_normalized = preprocessing.normalize(features.iloc[:,[1,2,3,4]])\n",
    "features_normalized = pd.DataFrame(features_normalized)\n",
    "features_normalized.columns = ['vis','r_fri_follow','avg_fav','frequency']\n",
    "features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a3bdd",
   "metadata": {},
   "source": [
    "### On effectue l'ACP pour réduire les 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f21f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features_standardized = StandardScaler().fit_transform(features.iloc[:,[1,2,3,4]]) #il faut centrer et réduire les données d'abord\n",
    "# vérification\n",
    "print(\"La moyenne : \",np.mean(features_standardized,axis=0))\n",
    "print(\"L'écart type : \",np.std(features_standardized,axis=0))\n",
    "\n",
    "acp = PCA(svd_solver='auto')\n",
    "coord = acp.fit_transform(features_standardized) \n",
    "print(acp.explained_variance_ratio_) # les pourcentage de variance projectée sur chaque axe (composante principale)\n",
    "coord # les coordonées des individus dans les nouveaux repères"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b464fc",
   "metadata": {},
   "source": [
    "## Unsupervised learning (K-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1ec89",
   "metadata": {},
   "source": [
    "### Basé sur le résultat d'ACP, on effectue le K-means dans le plan (1,2) qui se compose de deux composantes dont inerties les plus élevées \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ff509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0) # Selon le sujet, ici on ne choisit que de classer en 2 clusters.\n",
    "labels = kmeans.fit_predict(coord[:,[0,1]]) # on ne prend que les coordonées de l'axe 1 et 2\n",
    "\n",
    "print(np.unique(labels, return_counts=True)) # afficher la condition de séparation\n",
    "\n",
    "labels0 = coord[labels == 0]\n",
    "labels1 = coord[labels == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792850c1",
   "metadata": {},
   "source": [
    "### Plot K-means \n",
    "\n",
    "Pour voir plus clairement, on prend aléatoirement 100 points pour tracer le graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd7041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.random.choice(labels0[:,0],100), np.random.choice(labels0[:,1],100), c='red', s=5) \n",
    "\n",
    "plt.scatter(np.random.choice(labels1[:,0],100), np.random.choice(labels1[:,1],100), c='blue', s=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf900364",
   "metadata": {},
   "source": [
    "### Avant de passer à l'approche supervisée, il faut d'abord labeliser les données. Dans cette étape on va utiliser un échantillon de taille 50000 pour entraîner le modèle. Comme on a 4 indicateurs, on décide de les combiner en utilisant la méthode d'entropie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dcdbb",
   "metadata": {},
   "source": [
    "## Labeliser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86812181",
   "metadata": {},
   "source": [
    "### Méthode d'entropie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82985f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = features.iloc[:,[1,2,3,4]]\n",
    "indicateur = data.columns.tolist()   ## nombre des indicateurs\n",
    "profil = data.index.tolist()    ## nombre des profils\n",
    "value = data.values\n",
    "\n",
    "# définir la fonction de normalisation, on rajoute une constante 0,01\n",
    "def std_data(value,flag):\n",
    "    for i in range(len(indicateur)):\n",
    "        if flag[i]=='+':\n",
    "            value[:,i]=(value[:,i]-np.min(value[:,i],axis=0))/(np.max(value[:,i],axis=0)-np.min(value[:,i],axis=0))+0.01\n",
    "        elif flag[i]=='-':\n",
    "            value[:,i]=(np.max(value[:,i],axis=0)-value[:,i])/(np.max(value[:,i],axis=0)-np.min(value[:,i],axis=0))+0.01\n",
    "    return value\n",
    "\n",
    "# définir la fonction d'entropie et calculer les poids et les entropies\n",
    "def cal_weight(indicateur,profil,value):\n",
    "    p= np.array([[0.0 for i in range(len(indicateur))] for i in range(len(profil))])                    \n",
    "    for i in range(len(indicateur)):\n",
    "        p[:,i]=value[:,i]/np.sum(value[:,i],axis=0)\n",
    "        \n",
    "    e=-1/np.log(len(profil))*sum(p*np.log(p))      #calcul d'entropie\n",
    "    g=1-e     \n",
    "    w=g/sum(g)     #calcul des poids\n",
    "    return w\n",
    "\n",
    "# normalisation des données\n",
    "flag=[\"+\",\"+\",\"+\",\"+\"]  ## '+' ou '-' désigne le sens positif ou négatif des indicateurs\n",
    "std_value=std_data(value,flag)\n",
    "std_value.round(3)\n",
    "\n",
    "# Résultat\n",
    "w=cal_weight(indicateur,profil,std_value)\n",
    "w=pd.DataFrame(w,index=data.columns,columns=['Poids'])\n",
    "print(\"#######Poids:#######\")\n",
    "print(w)\n",
    "score=np.dot(std_value,w).round(2)\n",
    "score=pd.DataFrame(score,index=data.index,columns=['Notes']).sort_values(by =['Notes'],ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65736633",
   "metadata": {},
   "source": [
    "### la note de chaque utilisateur représente la correspondance avec les profils qu'on va détecter. Si une note plus élevée, le profil correspondant est plus probablement censé un influnceur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21359d24",
   "metadata": {},
   "source": [
    "### A l'aide de résultat ci-dessus, on peut compter la fréquence de chaque note. Avec cela, on va ensuite calculer le pourcentage de fréquence cumulée. En vertu de principe de Pareto (80/20), on peut alors découper les données en 2 classes: les profils plus générals (la somme de fréquence représente environ 80% au total) et le reste plus critique (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "data1 = []\n",
    "for i in range(266203):\n",
    "    data1.append(score.iloc[i,0])\n",
    "\n",
    "c = collections.Counter(data1) #créer un counter pour compter la fréquence de note\n",
    "\n",
    "freq = np.array(list(dict(c).items()),dtype = 'float')\n",
    "np.set_printoptions(suppress=True)\n",
    "freq = freq[np.lexsort(-freq.T)] # ordonner par décroissant en fonction de fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5587cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "somme = 0\n",
    "cumul = 0.00\n",
    "cluster = []\n",
    "\n",
    "for i in range(len(freq)):\n",
    "    if cumul <= 0.81:          # 0.01 de tolérance \n",
    "        somme += freq[i,1].astype(int)\n",
    "        cluster.append(freq[i,0])\n",
    "        cumul = float(somme/len(data1))\n",
    "    else: \n",
    "        break\n",
    "    \n",
    "print(cumul) # le pourcentage cumulé pour la classe 'générale'\n",
    "print(cluster) #les notes de cette classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cf1b2",
   "metadata": {},
   "source": [
    "### On va maintenant labeliser les profils en fonction de classes divisées ci-dessus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b55a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label = score.apply(lambda row: 1 if row[\"Notes\"] in cluster else -1, axis=1 )\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96360f44",
   "metadata": {},
   "source": [
    "## Supervised learning  (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c0119",
   "metadata": {},
   "source": [
    "### Application de la méthode SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d51210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features_normalized\n",
    "data[\"label\"] = label\n",
    "data = data.sample(50000) # On prend un échantillon de taille 50000\n",
    "\n",
    "X = data.iloc[:,[0,1,2,3]]\n",
    "y = data.iloc[:,[4]]\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                    test_size=0.3, random_state=42) # On sépare les jeux d'entrâinement et de test         \n",
    "\n",
    "\n",
    "parameters = {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.001,0.0001]}\n",
    "              \n",
    "             \n",
    "# svm_clf = svm.SVC()\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f1ff3",
   "metadata": {},
   "source": [
    "### Grid Search de parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd00852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(svm_clf, parameters)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# a=pd.DataFrame(clf.cv_results_)\n",
    "# a.sort_values(['mean_test_score'],ascending=False)\n",
    "\n",
    "\n",
    "# print(clf.best_estimator_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad6647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred),'\\n',classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4840b0",
   "metadata": {},
   "source": [
    "### On doit verifier si notre modele a overfit ou pas. Comme on le voit, puisque la precision de prediction est pareille sur X_train et sur X_test, on n'a pas le probleme de overfitting ici.\n",
    "\n",
    "### Donc, on n'a pas besoin de faire la regularization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_X_train = svm_clf.predict(X_train)\n",
    "print(accuracy_score(y_train,y_pred_X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b8488",
   "metadata": {},
   "source": [
    "### plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b87fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# print(cm_normalized)\n",
    "\n",
    "tick_marks = np.array(range(2)) + 0.5###\n",
    "\n",
    "labels=range(2)###\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.binary):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    xlocations = np.array(range(2))###\n",
    "    plt.xticks(xlocations, labels, rotation=90)\n",
    "    plt.yticks(xlocations, labels)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=120)\n",
    "\n",
    "ind_array = np.arange(2)###\n",
    "x, y = np.meshgrid(ind_array, ind_array)\n",
    "\n",
    "for x_val, y_val in zip(x.flatten(), y.flatten()):\n",
    "    c = cm_normalized[y_val][x_val]\n",
    "    if c > 0.01:\n",
    "        plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n",
    "# offset the tick\n",
    "plt.gca().set_xticks(tick_marks, minor=True)\n",
    "plt.gca().set_yticks(tick_marks, minor=True)\n",
    "plt.gca().xaxis.set_ticks_position('none')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.grid(True, which='minor', linestyle='-')\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
